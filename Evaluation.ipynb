{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (24.3.7)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.24.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: rich in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: optree in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from keras>=3.0.0->tensorflow) (0.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from rich->keras>=3.0.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# data = pd.read_csv('../data/reddit.csv')\n",
    "data_json = pd.read_json(\"../AwareData/reddit.json\")\n",
    "data=pd.DataFrame(data_json)\n",
    "data.loc[:, 'index'] = data.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = data.loc[data['aware_post_type']=='submission'].reset_index(drop=True)\n",
    "\n",
    "subs.loc[:, 'text_title'] = [str(subs['reddit_title'].iloc[i]) + ' [SEP] ' + str(subs['reddit_text'].iloc[i]) for i in range(len(subs))]\n",
    "subs.loc[:, 'text_title_subreddit'] = [str(subs['reddit_title'].iloc[i]) + ' ' + str(subs['reddit_text'].iloc[i]) + ' #' + str(subs['reddit_subreddit'].iloc[i]) for i in range(len(subs))]\n",
    "\n",
    "subs['comment_indices'] = [[] for _ in range(len(subs))]\n",
    "for i in range(len(subs)):\n",
    "    sub_index = subs.loc[i, 'index']\n",
    "    next_sub_index = len(data) if i == len(subs)-1 else subs.loc[i+1,'index']\n",
    "    for j in range(sub_index+1, next_sub_index):\n",
    "        if data.loc[j, 'reddit_parent_id'] == subs.loc[i, 'reddit_name']:\n",
    "            subs['comment_indices'].iloc[i].append(data.loc[j,'index'])\n",
    "\n",
    "subs = subs.drop(subs[subs['comment_indices'].str.len()==0].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "subs_train, subs_test = train_test_split(subs, test_size=100)\n",
    "\n",
    "subs_train = subs_train.reset_index(drop=True)\n",
    "subs_test = subs_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate test questions and ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = subs_test['text_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = [data.loc[subs_test.iloc[i]['comment_indices'][0], 'reddit_text'] for i in range(len(subs_test))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device)\n",
    "model.save(path='../emb/all-MiniLM-L6-v2', model_name='all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = model.encode(subs_train['text_title_subreddit'].values.tolist(),\n",
    "                       convert_to_numpy=True,\n",
    "                       normalize_embeddings=True)\n",
    "subs_train['vector'] = vectors.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lancedb\n",
    "\n",
    "db = lancedb.connect(\"../.lancedb\")\n",
    "table = db.create_table(\"reddit_submissions\", subs_train, mode=\"overwrite\")\n",
    "# table = db.create_table(\"reddit_submissions\", subs, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_num = 4\n",
    "contexts = [['' for j in range(context_num)] for i in range(len(subs_test))]\n",
    "\n",
    "for i in range(len(subs_test)):\n",
    "    query = model.encode(question[i],\n",
    "                       convert_to_numpy=True,\n",
    "                       normalize_embeddings=True).tolist()\n",
    "    response = table.search(query).limit(context_num).to_pandas()\n",
    "    comment_indices = [response.loc[j, 'comment_indices'] for j in range(context_num)]\n",
    "    for j in range(context_num):\n",
    "    # contexts.append([data.loc[comment_indices[j], 'reddit_text'] for j in range(context_num)])\n",
    "        contexts[i][j] = data.loc[comment_indices[j][0], 'reddit_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate answers using llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../AwareData/llama/llama-2-7b/llama-2-7b.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   140.56 MiB, ( 1272.22 / 49152.00)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4560.87 MiB\n",
      "llm_load_tensors:      Metal buffer size =   140.56 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 800\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Max\n",
      "ggml_metal_init: picking default device: Apple M3 Max\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M3 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 51539.61 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   387.50 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    12.50 MiB, ( 1284.97 / 49152.00)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    12.50 MiB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    10.57 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    83.58 MiB, ( 1368.55 / 49152.00)\n",
      "llama_new_context_with_model:      Metal compute buffer size =    83.56 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    90.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '17', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "generating_llm = Llama(\n",
    "#     model_path=\"../llm/llama-3-8b.Q5_K_M.gguf\",\n",
    "    model_path=\"../AwareData/llama/llama-2-7b/llama-2-7b.Q5_K_M.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=800,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.33 ms /    32 runs   (    0.07 ms per token, 13728.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1937.33 ms /   148 tokens (   13.09 ms per token,    76.39 tokens per second)\n",
      "llama_print_timings:        eval time =    1442.45 ms /    31 runs   (   46.53 ms per token,    21.49 tokens per second)\n",
      "llama_print_timings:       total time =    3418.72 ms /   179 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.20 ms /    32 runs   (    0.07 ms per token, 14565.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1394.42 ms /    73 tokens (   19.10 ms per token,    52.35 tokens per second)\n",
      "llama_print_timings:        eval time =    1390.98 ms /    31 runs   (   44.87 ms per token,    22.29 tokens per second)\n",
      "llama_print_timings:       total time =    2822.40 ms /   104 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.18 ms /    32 runs   (    0.07 ms per token, 14685.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1256.20 ms /    59 tokens (   21.29 ms per token,    46.97 tokens per second)\n",
      "llama_print_timings:        eval time =    1382.56 ms /    31 runs   (   44.60 ms per token,    22.42 tokens per second)\n",
      "llama_print_timings:       total time =    2676.22 ms /    90 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       1.36 ms /    17 runs   (    0.08 ms per token, 12463.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2483.33 ms /   257 tokens (    9.66 ms per token,   103.49 tokens per second)\n",
      "llama_print_timings:        eval time =     735.78 ms /    16 runs   (   45.99 ms per token,    21.75 tokens per second)\n",
      "llama_print_timings:       total time =    3239.07 ms /   273 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       1.38 ms /    20 runs   (    0.07 ms per token, 14461.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1269.29 ms /    64 tokens (   19.83 ms per token,    50.42 tokens per second)\n",
      "llama_print_timings:        eval time =     837.09 ms /    19 runs   (   44.06 ms per token,    22.70 tokens per second)\n",
      "llama_print_timings:       total time =    2127.92 ms /    83 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2258.60 ms /   228 tokens (    9.91 ms per token,   100.95 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2259.77 ms /   229 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.23 ms /    32 runs   (    0.07 ms per token, 14362.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2133.13 ms /   220 tokens (    9.70 ms per token,   103.13 tokens per second)\n",
      "llama_print_timings:        eval time =    1426.61 ms /    31 runs   (   46.02 ms per token,    21.73 tokens per second)\n",
      "llama_print_timings:       total time =    3597.07 ms /   251 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.21 ms /    32 runs   (    0.07 ms per token, 14446.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1235.36 ms /    60 tokens (   20.59 ms per token,    48.57 tokens per second)\n",
      "llama_print_timings:        eval time =    1366.57 ms /    31 runs   (   44.08 ms per token,    22.68 tokens per second)\n",
      "llama_print_timings:       total time =    2637.16 ms /    91 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.49 ms /     7 runs   (    0.07 ms per token, 14141.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2495.47 ms /   268 tokens (    9.31 ms per token,   107.39 tokens per second)\n",
      "llama_print_timings:        eval time =     271.26 ms /     6 runs   (   45.21 ms per token,    22.12 tokens per second)\n",
      "llama_print_timings:       total time =    2774.60 ms /   274 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.60 ms /     8 runs   (    0.08 ms per token, 13245.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2903.08 ms /   325 tokens (    8.93 ms per token,   111.95 tokens per second)\n",
      "llama_print_timings:        eval time =     318.99 ms /     7 runs   (   45.57 ms per token,    21.94 tokens per second)\n",
      "llama_print_timings:       total time =    3231.99 ms /   332 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.25 ms /    32 runs   (    0.07 ms per token, 14234.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1237.91 ms /    44 tokens (   28.13 ms per token,    35.54 tokens per second)\n",
      "llama_print_timings:        eval time =    1366.66 ms /    31 runs   (   44.09 ms per token,    22.68 tokens per second)\n",
      "llama_print_timings:       total time =    2640.49 ms /    75 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.70 ms /    10 runs   (    0.07 ms per token, 14388.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1410.27 ms /    85 tokens (   16.59 ms per token,    60.27 tokens per second)\n",
      "llama_print_timings:        eval time =     398.47 ms /     9 runs   (   44.27 ms per token,    22.59 tokens per second)\n",
      "llama_print_timings:       total time =    1819.52 ms /    94 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.23 ms /    32 runs   (    0.07 ms per token, 14317.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1224.57 ms /    58 tokens (   21.11 ms per token,    47.36 tokens per second)\n",
      "llama_print_timings:        eval time =    1363.60 ms /    31 runs   (   43.99 ms per token,    22.73 tokens per second)\n",
      "llama_print_timings:       total time =    2623.03 ms /    89 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.23 ms /    32 runs   (    0.07 ms per token, 14375.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1208.48 ms /    48 tokens (   25.18 ms per token,    39.72 tokens per second)\n",
      "llama_print_timings:        eval time =    1376.20 ms /    31 runs   (   44.39 ms per token,    22.53 tokens per second)\n",
      "llama_print_timings:       total time =    2619.27 ms /    79 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.21 ms /    32 runs   (    0.07 ms per token, 14466.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1554.91 ms /   103 tokens (   15.10 ms per token,    66.24 tokens per second)\n",
      "llama_print_timings:        eval time =    1390.24 ms /    31 runs   (   44.85 ms per token,    22.30 tokens per second)\n",
      "llama_print_timings:       total time =    2982.01 ms /   134 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.23 ms /    32 runs   (    0.07 ms per token, 14330.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2132.42 ms /   213 tokens (   10.01 ms per token,    99.89 tokens per second)\n",
      "llama_print_timings:        eval time =    1453.96 ms /    31 runs   (   46.90 ms per token,    21.32 tokens per second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_print_timings:       total time =    3625.42 ms /   244 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.29 ms /    32 runs   (    0.07 ms per token, 13992.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =     876.20 ms /    28 tokens (   31.29 ms per token,    31.96 tokens per second)\n",
      "llama_print_timings:        eval time =    1346.39 ms /    31 runs   (   43.43 ms per token,    23.02 tokens per second)\n",
      "llama_print_timings:       total time =    2259.78 ms /    59 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.26 ms /    32 runs   (    0.07 ms per token, 14165.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1367.31 ms /    78 tokens (   17.53 ms per token,    57.05 tokens per second)\n",
      "llama_print_timings:        eval time =    1378.60 ms /    31 runs   (   44.47 ms per token,    22.49 tokens per second)\n",
      "llama_print_timings:       total time =    2782.71 ms /   109 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12658.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2270.62 ms /   229 tokens (    9.92 ms per token,   100.85 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2271.83 ms /   230 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3357.54 ms /   366 tokens (    9.17 ms per token,   109.01 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    3358.92 ms /   367 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       1.38 ms /    20 runs   (    0.07 ms per token, 14471.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1578.65 ms /   124 tokens (   12.73 ms per token,    78.55 tokens per second)\n",
      "llama_print_timings:        eval time =     859.36 ms /    19 runs   (   45.23 ms per token,    22.11 tokens per second)\n",
      "llama_print_timings:       total time =    2461.57 ms /   143 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.19 ms /    32 runs   (    0.07 ms per token, 14631.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1378.79 ms /    65 tokens (   21.21 ms per token,    47.14 tokens per second)\n",
      "llama_print_timings:        eval time =    1373.67 ms /    31 runs   (   44.31 ms per token,    22.57 tokens per second)\n",
      "llama_print_timings:       total time =    2789.72 ms /    96 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.27 ms /     4 runs   (    0.07 ms per token, 14598.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1566.30 ms /   107 tokens (   14.64 ms per token,    68.31 tokens per second)\n",
      "llama_print_timings:        eval time =     135.54 ms /     3 runs   (   45.18 ms per token,    22.13 tokens per second)\n",
      "llama_print_timings:       total time =    1706.49 ms /   110 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2357.95 ms /   253 tokens (    9.32 ms per token,   107.30 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2359.16 ms /   254 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.29 ms /     4 runs   (    0.07 ms per token, 13698.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2687.77 ms /   302 tokens (    8.90 ms per token,   112.36 tokens per second)\n",
      "llama_print_timings:        eval time =     139.34 ms /     3 runs   (   46.45 ms per token,    21.53 tokens per second)\n",
      "llama_print_timings:       total time =    2831.89 ms /   305 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       1.32 ms /    18 runs   (    0.07 ms per token, 13595.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1513.14 ms /   104 tokens (   14.55 ms per token,    68.73 tokens per second)\n",
      "llama_print_timings:        eval time =     781.97 ms /    17 runs   (   46.00 ms per token,    21.74 tokens per second)\n",
      "llama_print_timings:       total time =    2316.06 ms /   121 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.34 ms /    32 runs   (    0.07 ms per token, 13669.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =     979.86 ms /    31 tokens (   31.61 ms per token,    31.64 tokens per second)\n",
      "llama_print_timings:        eval time =    1349.64 ms /    31 runs   (   43.54 ms per token,    22.97 tokens per second)\n",
      "llama_print_timings:       total time =    2366.84 ms /    62 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.72 ms /    32 runs   (    0.08 ms per token, 11773.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1378.20 ms /    88 tokens (   15.66 ms per token,    63.85 tokens per second)\n",
      "llama_print_timings:        eval time =    1386.99 ms /    31 runs   (   44.74 ms per token,    22.35 tokens per second)\n",
      "llama_print_timings:       total time =    2802.66 ms /   119 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.19 ms /    32 runs   (    0.07 ms per token, 14591.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1767.28 ms /   160 tokens (   11.05 ms per token,    90.53 tokens per second)\n",
      "llama_print_timings:        eval time =    1398.13 ms /    31 runs   (   45.10 ms per token,    22.17 tokens per second)\n",
      "llama_print_timings:       total time =    3202.83 ms /   191 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.21 ms /    32 runs   (    0.07 ms per token, 14486.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1923.46 ms /   186 tokens (   10.34 ms per token,    96.70 tokens per second)\n",
      "llama_print_timings:        eval time =    1402.19 ms /    31 runs   (   45.23 ms per token,    22.11 tokens per second)\n",
      "llama_print_timings:       total time =    3364.70 ms /   217 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.67 ms /    32 runs   (    0.08 ms per token, 11998.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2298.87 ms /   231 tokens (    9.95 ms per token,   100.48 tokens per second)\n",
      "llama_print_timings:        eval time =    1421.78 ms /    31 runs   (   45.86 ms per token,    21.80 tokens per second)\n",
      "llama_print_timings:       total time =    3758.09 ms /   262 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12658.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2296.78 ms /   232 tokens (    9.90 ms per token,   101.01 tokens per second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2298.01 ms /   233 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12987.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1913.91 ms /   181 tokens (   10.57 ms per token,    94.57 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1915.25 ms /   182 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1701.36 ms /   136 tokens (   12.51 ms per token,    79.94 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1702.12 ms /   137 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.14 ms /    32 runs   (    0.07 ms per token, 14967.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1917.58 ms /   185 tokens (   10.37 ms per token,    96.48 tokens per second)\n",
      "llama_print_timings:        eval time =    1413.63 ms /    31 runs   (   45.60 ms per token,    21.93 tokens per second)\n",
      "llama_print_timings:       total time =    3369.59 ms /   216 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13333.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1726.76 ms /   158 tokens (   10.93 ms per token,    91.50 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1727.52 ms /   159 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12345.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2290.80 ms /   248 tokens (    9.24 ms per token,   108.26 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2291.74 ms /   249 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.55 ms /     8 runs   (    0.07 ms per token, 14652.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1385.91 ms /    67 tokens (   20.69 ms per token,    48.34 tokens per second)\n",
      "llama_print_timings:        eval time =     315.40 ms /     7 runs   (   45.06 ms per token,    22.19 tokens per second)\n",
      "llama_print_timings:       total time =    1710.32 ms /    74 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3294.64 ms /   363 tokens (    9.08 ms per token,   110.18 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    3295.73 ms /   364 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.03 ms /    29 runs   (    0.07 ms per token, 14306.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2305.20 ms /   240 tokens (    9.60 ms per token,   104.11 tokens per second)\n",
      "llama_print_timings:        eval time =    1283.05 ms /    28 runs   (   45.82 ms per token,    21.82 tokens per second)\n",
      "llama_print_timings:       total time =    3622.22 ms /   268 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14705.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2118.57 ms /   203 tokens (   10.44 ms per token,    95.82 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2119.31 ms /   204 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13888.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1722.33 ms /   145 tokens (   11.88 ms per token,    84.19 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1723.67 ms /   146 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.25 ms /    32 runs   (    0.07 ms per token, 14222.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1550.23 ms /   107 tokens (   14.49 ms per token,    69.02 tokens per second)\n",
      "llama_print_timings:        eval time =    1400.27 ms /    31 runs   (   45.17 ms per token,    22.14 tokens per second)\n",
      "llama_print_timings:       total time =    2987.38 ms /   138 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       1.25 ms /    17 runs   (    0.07 ms per token, 13621.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1560.62 ms /   114 tokens (   13.69 ms per token,    73.05 tokens per second)\n",
      "llama_print_timings:        eval time =     714.66 ms /    16 runs   (   44.67 ms per token,    22.39 tokens per second)\n",
      "llama_print_timings:       total time =    2294.14 ms /   130 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.17 ms /    32 runs   (    0.07 ms per token, 14719.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1914.66 ms /   161 tokens (   11.89 ms per token,    84.09 tokens per second)\n",
      "llama_print_timings:        eval time =    1395.77 ms /    31 runs   (   45.02 ms per token,    22.21 tokens per second)\n",
      "llama_print_timings:       total time =    3347.31 ms /   192 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.35 ms /    32 runs   (    0.07 ms per token, 13611.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1083.48 ms /    32 tokens (   33.86 ms per token,    29.53 tokens per second)\n",
      "llama_print_timings:        eval time =    1360.89 ms /    31 runs   (   43.90 ms per token,    22.78 tokens per second)\n",
      "llama_print_timings:       total time =    2481.17 ms /    63 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.27 ms /    32 runs   (    0.07 ms per token, 14109.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2283.59 ms /   249 tokens (    9.17 ms per token,   109.04 tokens per second)\n",
      "llama_print_timings:        eval time =    1436.07 ms /    31 runs   (   46.32 ms per token,    21.59 tokens per second)\n",
      "llama_print_timings:       total time =    3757.34 ms /   280 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 10989.01 tokens per second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_print_timings: prompt eval time =    2070.30 ms /   196 tokens (   10.56 ms per token,    94.67 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2071.48 ms /   197 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2683.33 ms /   309 tokens (    8.68 ms per token,   115.16 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2684.73 ms /   310 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.19 ms /    32 runs   (    0.07 ms per token, 14585.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1426.71 ms /    93 tokens (   15.34 ms per token,    65.18 tokens per second)\n",
      "llama_print_timings:        eval time =    1394.56 ms /    31 runs   (   44.99 ms per token,    22.23 tokens per second)\n",
      "llama_print_timings:       total time =    2857.42 ms /   124 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.27 ms /    32 runs   (    0.07 ms per token, 14090.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1433.67 ms /    96 tokens (   14.93 ms per token,    66.96 tokens per second)\n",
      "llama_print_timings:        eval time =    1390.59 ms /    31 runs   (   44.86 ms per token,    22.29 tokens per second)\n",
      "llama_print_timings:       total time =    2859.85 ms /   127 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.21 ms /    32 runs   (    0.07 ms per token, 14460.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1353.77 ms /    72 tokens (   18.80 ms per token,    53.18 tokens per second)\n",
      "llama_print_timings:        eval time =    1384.51 ms /    31 runs   (   44.66 ms per token,    22.39 tokens per second)\n",
      "llama_print_timings:       total time =    2774.89 ms /   103 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 14084.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1720.31 ms /   134 tokens (   12.84 ms per token,    77.89 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1721.47 ms /   135 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.19 ms /    32 runs   (    0.07 ms per token, 14585.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1199.62 ms /    47 tokens (   25.52 ms per token,    39.18 tokens per second)\n",
      "llama_print_timings:        eval time =    1367.19 ms /    31 runs   (   44.10 ms per token,    22.67 tokens per second)\n",
      "llama_print_timings:       total time =    2603.66 ms /    78 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.65 ms /     8 runs   (    0.08 ms per token, 12364.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =     547.28 ms /    18 tokens (   30.40 ms per token,    32.89 tokens per second)\n",
      "llama_print_timings:        eval time =     303.07 ms /     7 runs   (   43.30 ms per token,    23.10 tokens per second)\n",
      "llama_print_timings:       total time =     859.43 ms /    25 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.18 ms /    32 runs   (    0.07 ms per token, 14652.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1413.86 ms /    86 tokens (   16.44 ms per token,    60.83 tokens per second)\n",
      "llama_print_timings:        eval time =    1381.16 ms /    31 runs   (   44.55 ms per token,    22.44 tokens per second)\n",
      "llama_print_timings:       total time =    2831.98 ms /   117 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.09 ms /     1 runs   (    0.09 ms per token, 10989.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2487.34 ms /   274 tokens (    9.08 ms per token,   110.16 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2488.69 ms /   275 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.24 ms /    32 runs   (    0.07 ms per token, 14260.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1378.93 ms /    69 tokens (   19.98 ms per token,    50.04 tokens per second)\n",
      "llama_print_timings:        eval time =    1391.86 ms /    31 runs   (   44.90 ms per token,    22.27 tokens per second)\n",
      "llama_print_timings:       total time =    2807.26 ms /   100 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.85 ms /    11 runs   (    0.08 ms per token, 12971.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =     841.17 ms /    28 tokens (   30.04 ms per token,    33.29 tokens per second)\n",
      "llama_print_timings:        eval time =     435.16 ms /    10 runs   (   43.52 ms per token,    22.98 tokens per second)\n",
      "llama_print_timings:       total time =    1288.86 ms /    38 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       1.51 ms /    20 runs   (    0.08 ms per token, 13280.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     868.39 ms /    29 tokens (   29.94 ms per token,    33.40 tokens per second)\n",
      "llama_print_timings:        eval time =     844.05 ms /    19 runs   (   44.42 ms per token,    22.51 tokens per second)\n",
      "llama_print_timings:       total time =    1734.96 ms /    48 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     4 runs   (    0.08 ms per token, 13157.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1230.05 ms /    49 tokens (   25.10 ms per token,    39.84 tokens per second)\n",
      "llama_print_timings:        eval time =     131.50 ms /     3 runs   (   43.83 ms per token,    22.81 tokens per second)\n",
      "llama_print_timings:       total time =    1365.84 ms /    52 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.93 ms /    13 runs   (    0.07 ms per token, 13948.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1538.40 ms /   111 tokens (   13.86 ms per token,    72.15 tokens per second)\n",
      "llama_print_timings:        eval time =     537.37 ms /    12 runs   (   44.78 ms per token,    22.33 tokens per second)\n",
      "llama_print_timings:       total time =    2091.44 ms /   123 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.23 ms /    32 runs   (    0.07 ms per token, 14375.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1529.09 ms /   119 tokens (   12.85 ms per token,    77.82 tokens per second)\n",
      "llama_print_timings:        eval time =    1409.63 ms /    31 runs   (   45.47 ms per token,    21.99 tokens per second)\n",
      "llama_print_timings:       total time =    2976.91 ms /   150 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_print_timings:      sample time =       2.15 ms /    32 runs   (    0.07 ms per token, 14856.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2290.05 ms /   228 tokens (   10.04 ms per token,    99.56 tokens per second)\n",
      "llama_print_timings:        eval time =    1415.07 ms /    31 runs   (   45.65 ms per token,    21.91 tokens per second)\n",
      "llama_print_timings:       total time =    3743.07 ms /   259 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1938.05 ms /   192 tokens (   10.09 ms per token,    99.07 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1938.70 ms /   193 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.17 ms /    32 runs   (    0.07 ms per token, 14719.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1417.49 ms /    94 tokens (   15.08 ms per token,    66.31 tokens per second)\n",
      "llama_print_timings:        eval time =    1398.24 ms /    31 runs   (   45.10 ms per token,    22.17 tokens per second)\n",
      "llama_print_timings:       total time =    2853.79 ms /   125 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.26 ms /    32 runs   (    0.07 ms per token, 14159.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1524.93 ms /   109 tokens (   13.99 ms per token,    71.48 tokens per second)\n",
      "llama_print_timings:        eval time =    1394.58 ms /    31 runs   (   44.99 ms per token,    22.23 tokens per second)\n",
      "llama_print_timings:       total time =    2955.60 ms /   140 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12658.23 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1704.67 ms /   134 tokens (   12.72 ms per token,    78.61 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1705.14 ms /   135 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13513.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3537.59 ms /   403 tokens (    8.78 ms per token,   113.92 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    3538.94 ms /   404 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.07 ms /     1 runs   (    0.07 ms per token, 13698.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2439.34 ms /   257 tokens (    9.49 ms per token,   105.36 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2440.37 ms /   258 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.17 ms /    32 runs   (    0.07 ms per token, 14732.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1249.23 ms /    48 tokens (   26.03 ms per token,    38.42 tokens per second)\n",
      "llama_print_timings:        eval time =    1368.81 ms /    31 runs   (   44.16 ms per token,    22.65 tokens per second)\n",
      "llama_print_timings:       total time =    2654.33 ms /    79 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.21 ms /    32 runs   (    0.07 ms per token, 14446.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1365.63 ms /    82 tokens (   16.65 ms per token,    60.05 tokens per second)\n",
      "llama_print_timings:        eval time =    1396.09 ms /    31 runs   (   45.04 ms per token,    22.20 tokens per second)\n",
      "llama_print_timings:       total time =    2798.10 ms /   113 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.27 ms /    32 runs   (    0.07 ms per token, 14121.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1587.87 ms /   122 tokens (   13.02 ms per token,    76.83 tokens per second)\n",
      "llama_print_timings:        eval time =    1390.81 ms /    31 runs   (   44.86 ms per token,    22.29 tokens per second)\n",
      "llama_print_timings:       total time =    3015.51 ms /   153 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.24 ms /    32 runs   (    0.07 ms per token, 14298.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1209.81 ms /    54 tokens (   22.40 ms per token,    44.64 tokens per second)\n",
      "llama_print_timings:        eval time =    1374.73 ms /    31 runs   (   44.35 ms per token,    22.55 tokens per second)\n",
      "llama_print_timings:       total time =    2620.53 ms /    85 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.30 ms /     4 runs   (    0.08 ms per token, 13157.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1244.83 ms /    51 tokens (   24.41 ms per token,    40.97 tokens per second)\n",
      "llama_print_timings:        eval time =     135.99 ms /     3 runs   (   45.33 ms per token,    22.06 tokens per second)\n",
      "llama_print_timings:       total time =    1385.12 ms /    54 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.21 ms /    32 runs   (    0.07 ms per token, 14466.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1388.11 ms /    86 tokens (   16.14 ms per token,    61.95 tokens per second)\n",
      "llama_print_timings:        eval time =    1409.00 ms /    31 runs   (   45.45 ms per token,    22.00 tokens per second)\n",
      "llama_print_timings:       total time =    2833.36 ms /   117 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12987.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2486.96 ms /   263 tokens (    9.46 ms per token,   105.75 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2488.05 ms /   264 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.10 ms /     1 runs   (    0.10 ms per token, 10526.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2547.53 ms /   286 tokens (    8.91 ms per token,   112.27 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2548.72 ms /   287 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.21 ms /    32 runs   (    0.07 ms per token, 14512.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1875.93 ms /   161 tokens (   11.65 ms per token,    85.82 tokens per second)\n",
      "llama_print_timings:        eval time =    1394.10 ms /    31 runs   (   44.97 ms per token,    22.24 tokens per second)\n",
      "llama_print_timings:       total time =    3309.13 ms /   192 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.28 ms /    32 runs   (    0.07 ms per token, 14035.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1728.36 ms /   159 tokens (   10.87 ms per token,    91.99 tokens per second)\n",
      "llama_print_timings:        eval time =    1392.93 ms /    31 runs   (   44.93 ms per token,    22.26 tokens per second)\n",
      "llama_print_timings:       total time =    3159.95 ms /   190 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.33 ms /    32 runs   (    0.07 ms per token, 13757.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1194.86 ms /    45 tokens (   26.55 ms per token,    37.66 tokens per second)\n",
      "llama_print_timings:        eval time =    1367.59 ms /    31 runs   (   44.12 ms per token,    22.67 tokens per second)\n",
      "llama_print_timings:       total time =    2600.69 ms /    76 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.27 ms /    32 runs   (    0.07 ms per token, 14096.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1922.41 ms /   175 tokens (   10.99 ms per token,    91.03 tokens per second)\n",
      "llama_print_timings:        eval time =    1420.05 ms /    31 runs   (   45.81 ms per token,    21.83 tokens per second)\n",
      "llama_print_timings:       total time =    3379.92 ms /   206 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12987.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1916.09 ms /   186 tokens (   10.30 ms per token,    97.07 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    1916.69 ms /   187 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.36 ms /     5 runs   (    0.07 ms per token, 13927.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1545.86 ms /    97 tokens (   15.94 ms per token,    62.75 tokens per second)\n",
      "llama_print_timings:        eval time =     180.49 ms /     4 runs   (   45.12 ms per token,    22.16 tokens per second)\n",
      "llama_print_timings:       total time =    1731.97 ms /   101 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.19 ms /    32 runs   (    0.07 ms per token, 14631.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2073.27 ms /   196 tokens (   10.58 ms per token,    94.54 tokens per second)\n",
      "llama_print_timings:        eval time =    1436.48 ms /    31 runs   (   46.34 ms per token,    21.58 tokens per second)\n",
      "llama_print_timings:       total time =    3549.00 ms /   227 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.58 ms /     8 runs   (    0.07 ms per token, 13745.70 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2269.42 ms /   233 tokens (    9.74 ms per token,   102.67 tokens per second)\n",
      "llama_print_timings:        eval time =     317.06 ms /     7 runs   (   45.29 ms per token,    22.08 tokens per second)\n",
      "llama_print_timings:       total time =    2596.19 ms /   240 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.25 ms /    32 runs   (    0.07 ms per token, 14234.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3770.49 ms /   432 tokens (    8.73 ms per token,   114.57 tokens per second)\n",
      "llama_print_timings:        eval time =    1438.60 ms /    31 runs   (   46.41 ms per token,    21.55 tokens per second)\n",
      "llama_print_timings:       total time =    5250.13 ms /   463 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.57 ms /     8 runs   (    0.07 ms per token, 13937.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1414.90 ms /    91 tokens (   15.55 ms per token,    64.32 tokens per second)\n",
      "llama_print_timings:        eval time =     314.06 ms /     7 runs   (   44.87 ms per token,    22.29 tokens per second)\n",
      "llama_print_timings:       total time =    1737.60 ms /    98 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.21 ms /    32 runs   (    0.07 ms per token, 14446.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1227.29 ms /    64 tokens (   19.18 ms per token,    52.15 tokens per second)\n",
      "llama_print_timings:        eval time =    1375.88 ms /    31 runs   (   44.38 ms per token,    22.53 tokens per second)\n",
      "llama_print_timings:       total time =    2639.98 ms /    95 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.26 ms /    32 runs   (    0.07 ms per token, 14171.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1725.53 ms /   138 tokens (   12.50 ms per token,    79.98 tokens per second)\n",
      "llama_print_timings:        eval time =    1422.30 ms /    31 runs   (   45.88 ms per token,    21.80 tokens per second)\n",
      "llama_print_timings:       total time =    3185.26 ms /   169 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.25 ms /    32 runs   (    0.07 ms per token, 14241.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1215.37 ms /    53 tokens (   22.93 ms per token,    43.61 tokens per second)\n",
      "llama_print_timings:        eval time =    1376.65 ms /    31 runs   (   44.41 ms per token,    22.52 tokens per second)\n",
      "llama_print_timings:       total time =    2628.26 ms /    84 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.26 ms /    32 runs   (    0.07 ms per token, 14165.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2074.53 ms /   194 tokens (   10.69 ms per token,    93.51 tokens per second)\n",
      "llama_print_timings:        eval time =    1436.13 ms /    31 runs   (   46.33 ms per token,    21.59 tokens per second)\n",
      "llama_print_timings:       total time =    3548.38 ms /   225 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.12 ms /    32 runs   (    0.07 ms per token, 15115.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2491.03 ms /   266 tokens (    9.36 ms per token,   106.78 tokens per second)\n",
      "llama_print_timings:        eval time =    1410.55 ms /    31 runs   (   45.50 ms per token,    21.98 tokens per second)\n",
      "llama_print_timings:       total time =    3939.93 ms /   297 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.08 ms /     1 runs   (    0.08 ms per token, 12820.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2709.91 ms /   313 tokens (    8.66 ms per token,   115.50 tokens per second)\n",
      "llama_print_timings:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:       total time =    2710.55 ms /   314 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.37 ms /    32 runs   (    0.07 ms per token, 13519.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1197.07 ms /    43 tokens (   27.84 ms per token,    35.92 tokens per second)\n",
      "llama_print_timings:        eval time =    1366.27 ms /    31 runs   (   44.07 ms per token,    22.69 tokens per second)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_print_timings:       total time =    2599.79 ms /    74 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       0.29 ms /     4 runs   (    0.07 ms per token, 14035.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1204.58 ms /    38 tokens (   31.70 ms per token,    31.55 tokens per second)\n",
      "llama_print_timings:        eval time =     131.05 ms /     3 runs   (   43.68 ms per token,    22.89 tokens per second)\n",
      "llama_print_timings:       total time =    1339.42 ms /    41 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       1.30 ms /    18 runs   (    0.07 ms per token, 13846.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1204.43 ms /    47 tokens (   25.63 ms per token,    39.02 tokens per second)\n",
      "llama_print_timings:        eval time =     743.37 ms /    17 runs   (   43.73 ms per token,    22.87 tokens per second)\n",
      "llama_print_timings:       total time =    1967.08 ms /    64 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.23 ms /    32 runs   (    0.07 ms per token, 14317.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1920.19 ms /   172 tokens (   11.16 ms per token,    89.57 tokens per second)\n",
      "llama_print_timings:        eval time =    1431.72 ms /    31 runs   (   46.18 ms per token,    21.65 tokens per second)\n",
      "llama_print_timings:       total time =    3388.77 ms /   203 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.42 ms /    32 runs   (    0.08 ms per token, 13206.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1357.38 ms /    77 tokens (   17.63 ms per token,    56.73 tokens per second)\n",
      "llama_print_timings:        eval time =    1385.96 ms /    31 runs   (   44.71 ms per token,    22.37 tokens per second)\n",
      "llama_print_timings:       total time =    2779.86 ms /   108 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1937.43 ms\n",
      "llama_print_timings:      sample time =       2.26 ms /    32 runs   (    0.07 ms per token, 14146.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1700.17 ms /   146 tokens (   11.65 ms per token,    85.87 tokens per second)\n",
      "llama_print_timings:        eval time =    1395.67 ms /    31 runs   (   45.02 ms per token,    22.21 tokens per second)\n",
      "llama_print_timings:       total time =    3135.10 ms /   177 tokens\n"
     ]
    }
   ],
   "source": [
    "answer = []\n",
    "\n",
    "cutoff = 800\n",
    "\n",
    "for i in range(len(subs_test)):\n",
    "      context = contexts[i][0][:cutoff]\n",
    "      prompt = \"Summarize the following, \" + str(context) + \"Then answer question: \" + str(question[i][:800]) + \"Answer: \"\n",
    "      # prompt = 'Q:' + str(question[i]) + \"A: \"\n",
    "      output = generating_llm(prompt,\n",
    "            max_tokens=32, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "            stop=[\"\\n\"], # Stop generating just before the model would generate a new question\n",
    "            echo=False # Echo the prompt back in the output\n",
    "      )\n",
    "      answer.append(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from ../AwareData/llama/llama-2-7b/llama-2-7b.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 4.45 GiB (5.68 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =   140.56 MiB, ( 1461.11 / 49152.00)\n",
      "llm_load_tensors: offloading 1 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 1/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4560.87 MiB\n",
      "llm_load_tensors:      Metal buffer size =   140.56 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 800\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Max\n",
      "ggml_metal_init: picking default device: Apple M3 Max\n",
      "ggml_metal_init: default.metallib not found, loading from source\n",
      "ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil\n",
      "ggml_metal_init: loading '/Users/nessmaykerchen/anaconda3/lib/python3.11/site-packages/llama_cpp/ggml-metal.metal'\n",
      "ggml_metal_init: GPU name:   Apple M3 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 51539.61 MB\n",
      "llama_kv_cache_init:        CPU KV buffer size =   387.50 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    12.50 MiB, ( 1473.61 / 49152.00)\n",
      "llama_kv_cache_init:      Metal KV buffer size =    12.50 MiB\n",
      "llama_new_context_with_model: KV self size  =  400.00 MiB, K (f16):  200.00 MiB, V (f16):  200.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    10.57 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =    83.58 MiB, ( 1557.19 / 49152.00)\n",
      "llama_new_context_with_model:      Metal compute buffer size =    83.56 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    90.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 3\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '17', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "\n",
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "evaluating_llm = LlamaCpp(\n",
    "#     model_path=\"../llm/llama-3-8b.Q5_K_M.gguf\",\n",
    "    model_path=\"../AwareData/llama/llama-2-7b/llama-2-7b.Q5_K_M.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=800,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "evaluating_llm = LangchainLLMWrapper(evaluating_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "evaluating_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"../emb/all-MiniLM-L6-v2\",     # Provide the pre-trained model's path\n",
    "    model_kwargs={'device': device}, # Pass the model configuration options\n",
    "    encode_kwargs={'normalize_embeddings': True} # Pass the encoding options\n",
    ")\n",
    "\n",
    "evaluating_embeddings = LangchainEmbeddingsWrapper(evaluating_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "data_samples = {'question': [q[:cutoff] for q in question], 'contexts': [[c[:cutoff] for c in context]for context in contexts], 'ground_truth': [g[:cutoff] for g in ground_truth], 'answer': [a[:cutoff] for a in answer]}\n",
    "dataset = Dataset.from_dict(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231c56fae48e43b4840fbbf8f5a42ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = evaluate(\n",
    "    dataset,\n",
    "    llm = evaluating_llm,\n",
    "    embeddings=evaluating_embeddings,\n",
    "    metrics=[\n",
    "        answer_relevancy,\n",
    "        faithfulness,\n",
    "        context_recall,\n",
    "        context_precision,\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
